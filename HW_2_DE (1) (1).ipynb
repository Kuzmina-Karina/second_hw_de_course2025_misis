{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19efae20",
   "metadata": {
    "id": "19efae20"
   },
   "source": [
    "**1. Смотрим на Hadoop Distributed File System**\n",
    "\n",
    "В рамках этой части вам нужно будет обращаться к HDFS с помощью CLI, разместить файлы для следующих заданий в распределеннй файловой системе и выполнить несколько преобразований над ними.\n",
    "\n",
    "Для работы файлы можно скачать по следующим ссылкам:\n",
    "- Логи посещения сайтов юзерами за некоторый промежуток времени [ссылка](https://drive.google.com/file/d/1WXyq5WVSWwJYXPuH4kyAJ5mrR3XgfO_H/view?usp=sharing)\n",
    "\n",
    "Разместите их в нашем внутреннем файловом хранилище с помощью HDFS CLI, для дальнейшего удобства под каждый файл стоит создать каталог с простым и понятным именем, разместить сами файлы в разных каталогах.\n",
    "\n",
    "Набор комманд, которые вам могут в этом помочь, доступны [здесь](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html)\n",
    "\n",
    "В ячейках ниже должен быть полный набор комманд ваших обращей к консоли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0f990",
   "metadata": {
    "id": "b4c0f990",
    "outputId": "4b0cd4da-bd10-4619-ac02-cda922c3fd64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-01 07:40 /input\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-01 07:42 /output\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-01 07:34 /tmp\r\n"
     ]
    }
   ],
   "source": [
    "## вы можете обращаться к консоли из ноутбука таким способом\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5a267",
   "metadata": {
    "id": "41a5a267",
    "outputId": "33b4bc5e-9527-4241-a541-83ae98ce3891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-01 07:40 /input\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-01 07:42 /output\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-01 07:34 /tmp\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## или же использовать для этого меджик строчку в ячейке %%bash, как вам будет удобнее\n",
    "\n",
    "hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61786cf4",
   "metadata": {
    "id": "61786cf4",
    "outputId": "51760388-8777-4b84-c564-d28e769ba12f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup         24 2024-06-01 07:40 /input/input.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R -h /input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6e16b",
   "metadata": {
    "id": "a0c6e16b"
   },
   "outputs": [],
   "source": [
    "## ваше решение здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36584e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir /user/ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6d6ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -test -d /user/ubuntu/nomer1 || hdfs dfs -mkdir /user/ubuntu/nomer1\n",
    "hdfs dfs -test -f /user/ubuntu/nomer1/bible.txt || hdfs dfs -put ./bible.txt /user/ubuntu/nomer1/bible.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc46c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -test -d /user/ubuntu/nomer2 || hdfs dfs -mkdir /user/ubuntu/nomer2\n",
    "hdfs dfs -test -f /user/ubuntu/nomer2/Посещения_сайтов.csv || hdfs dfs -put ./Посещения_сайтов.csv /user/ubuntu/nomer2/Посещения_сайтов.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f83ac",
   "metadata": {
    "id": "1e1f83ac"
   },
   "source": [
    "**2. Решаем задачи MapReduce**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990332ef",
   "metadata": {
    "id": "990332ef"
   },
   "source": [
    "**2.1 Подсчет слов в тексте**\n",
    "\n",
    "В рамках данного задания вам нужно подсчитать кол-во слов в тексте Библии (файл приложен к ДЗ в чате тг), то есть необходимо реализовать базовый функционал утилиты word count.\n",
    "\n",
    "**Важно** - подсчитывайте число только тех слов, длина которых больше 4 символов. Проводить процесс удаления знаков препинания и прочих символов **не нужно**\n",
    "\n",
    "Ниже вам представлены ячейки, в которых вы должны описать структуру маппера/редьсюера и ниже вызвать их в bash-скрипте запуска MR-таски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295c61fd",
   "metadata": {
    "id": "295c61fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "import sys\n",
    "\n",
    "for stroka in sys.stdin:\n",
    "    stroka = stroka.strip()\n",
    "    if stroka:\n",
    "        slova = stroka.split()\n",
    "        for word in slova:\n",
    "            if len(word)>4:\n",
    "                print(word.lower() + str('\\t1'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1bef060",
   "metadata": {
    "id": "c1bef060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "import sys\n",
    "\n",
    "ans_key = None\n",
    "ans_count = 0\n",
    "\n",
    "for stroka in sys.stdin:\n",
    "    stroka = stroka.strip()\n",
    "    if stroka:\n",
    "        key, count = stroka.split('\\t', 1)\n",
    "        count = int(count) \n",
    "        \n",
    "    if ans_key == key:\n",
    "        ans_count += 1\n",
    "    else:\n",
    "        if ans_key:\n",
    "            print(ans_key + str('\\t') + str(ans_count))\n",
    "        ans_count = 1\n",
    "        ans_key = key\n",
    "        \n",
    "print(ans_key + str('\\t') + str(ans_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b35cd9",
   "metadata": {
    "id": "97b35cd9"
   },
   "source": [
    "В качестве проверки ваших python-скриптов до запуска MR таски можно произвести их запуск через консольные команды\n",
    "\n",
    "Тогда наша задача не будет выполняться через датаноды, а только на локальной машине, но в случае ошибок в скриптах вы увидите их и сможете исправить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5975e36",
   "metadata": {
    "id": "a5975e36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall\t9733\n",
      "which\t4244\n",
      "their\t3859\n",
      "there\t2008\n",
      "before\t1722\n",
      "lord,\t1613\n",
      "against\t1596\n",
      "shalt\t1589\n",
      "children\t1560\n",
      "said,\t1556\n",
      "them,\t1549\n",
      "saying,\t1272\n",
      "house\t1222\n",
      "every\t1208\n",
      "people\t1194\n",
      "because\t1178\n",
      "thee,\t1170\n",
      "these\t1147\n",
      "saith\t1135\n",
      "after\t1128\n",
      "behold,\t1073\n",
      "therefore\t1054\n",
      "israel\t1025\n",
      "among\t907\n",
      "thine\t883\n",
      "neither\t861\n",
      "great\t847\n",
      "brought\t815\n",
      "things\t780\n",
      "jesus\t777\n",
      "should\t772\n",
      "according\t755\n",
      "israel,\t724\n",
      "forth\t720\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## пример запуска скриптов на неймноде для проверки их работы\n",
    "\n",
    "cat bible.txt | python3 mapper.py | sort -k1,1 | python3 reducer.py | sort -k2,2nr | head -n 34\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279f6c1",
   "metadata": {
    "id": "8279f6c1"
   },
   "source": [
    "Как только в данной проверке вы получите успешный и корректный результат, можете запустить Map Reduce в ячейке ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c71eff58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob3744988546770856471.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `/user/ubuntu/nomer1/word_count_task': No such file or directory\n",
      "2025-04-16 08:00:24,964 INFO client.RMProxy: Connecting to ResourceManager at rc1d-dataproc-m-8hvn1jfvk1pddbqe.mdb.yandexcloud.net/10.130.0.11:8032\n",
      "2025-04-16 08:00:25,211 INFO client.AHSProxy: Connecting to Application History server at rc1d-dataproc-m-8hvn1jfvk1pddbqe.mdb.yandexcloud.net/10.130.0.11:10200\n",
      "2025-04-16 08:00:25,258 INFO client.RMProxy: Connecting to ResourceManager at rc1d-dataproc-m-8hvn1jfvk1pddbqe.mdb.yandexcloud.net/10.130.0.11:8032\n",
      "2025-04-16 08:00:25,259 INFO client.AHSProxy: Connecting to Application History server at rc1d-dataproc-m-8hvn1jfvk1pddbqe.mdb.yandexcloud.net/10.130.0.11:10200\n",
      "2025-04-16 08:00:25,488 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1744790037149_0006\n",
      "2025-04-16 08:00:26,455 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-04-16 08:00:26,643 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2025-04-16 08:00:26,808 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1744790037149_0006\n",
      "2025-04-16 08:00:26,810 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-04-16 08:00:27,069 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-04-16 08:00:27,069 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-04-16 08:00:27,147 INFO impl.YarnClientImpl: Submitted application application_1744790037149_0006\n",
      "2025-04-16 08:00:27,222 INFO mapreduce.Job: The url to track the job: http://rc1d-dataproc-m-8hvn1jfvk1pddbqe.mdb.yandexcloud.net:8088/proxy/application_1744790037149_0006/\n",
      "2025-04-16 08:00:27,224 INFO mapreduce.Job: Running job: job_1744790037149_0006\n",
      "2025-04-16 08:00:34,351 INFO mapreduce.Job: Job job_1744790037149_0006 running in uber mode : false\n",
      "2025-04-16 08:00:34,352 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-04-16 08:00:39,416 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2025-04-16 08:00:42,439 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "2025-04-16 08:00:43,445 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "2025-04-16 08:00:44,453 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2025-04-16 08:00:45,461 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2025-04-16 08:00:48,482 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-04-16 08:00:49,487 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2025-04-16 08:00:50,493 INFO mapreduce.Job:  map 47% reduce 0%\n",
      "2025-04-16 08:00:53,509 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2025-04-16 08:00:55,520 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "2025-04-16 08:00:56,526 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2025-04-16 08:00:58,547 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "2025-04-16 08:00:59,554 INFO mapreduce.Job:  map 67% reduce 2%\n",
      "2025-04-16 08:01:00,559 INFO mapreduce.Job:  map 73% reduce 4%\n",
      "2025-04-16 08:01:01,564 INFO mapreduce.Job:  map 73% reduce 5%\n",
      "2025-04-16 08:01:03,573 INFO mapreduce.Job:  map 77% reduce 5%\n",
      "2025-04-16 08:01:04,577 INFO mapreduce.Job:  map 83% reduce 5%\n",
      "2025-04-16 08:01:05,582 INFO mapreduce.Job:  map 87% reduce 8%\n",
      "2025-04-16 08:01:06,586 INFO mapreduce.Job:  map 87% reduce 9%\n",
      "2025-04-16 08:01:07,591 INFO mapreduce.Job:  map 90% reduce 12%\n",
      "2025-04-16 08:01:09,601 INFO mapreduce.Job:  map 100% reduce 12%\n",
      "2025-04-16 08:01:10,609 INFO mapreduce.Job:  map 100% reduce 24%\n",
      "2025-04-16 08:01:11,614 INFO mapreduce.Job:  map 100% reduce 58%\n",
      "2025-04-16 08:01:12,621 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2025-04-16 08:01:13,625 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "2025-04-16 08:01:14,630 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-04-16 08:01:15,640 INFO mapreduce.Job: Job job_1744790037149_0006 completed successfully\n",
      "2025-04-16 08:01:15,734 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=170689\n",
      "\t\tFILE: Number of bytes written=10928266\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7737301\n",
      "\t\tHDFS: Number of bytes written=276444\n",
      "\t\tHDFS: Number of read operations=150\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=36\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=12\n",
      "\t\tData-local map tasks=20\n",
      "\t\tRack-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=353484\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=411882\n",
      "\t\tTotal time spent by all map tasks (ms)=117828\n",
      "\t\tTotal time spent by all reduce tasks (ms)=137294\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=117828\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=137294\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=361967616\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=421767168\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=30383\n",
      "\t\tMap output records=266384\n",
      "\t\tMap output bytes=2548937\n",
      "\t\tMap output materialized bytes=558197\n",
      "\t\tInput split bytes=4200\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=25424\n",
      "\t\tReduce shuffle bytes=558197\n",
      "\t\tReduce input records=266384\n",
      "\t\tReduce output records=25424\n",
      "\t\tSpilled Records=532768\n",
      "\t\tShuffled Maps =360\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=360\n",
      "\t\tGC time elapsed (ms)=4274\n",
      "\t\tCPU time spent (ms)=46490\n",
      "\t\tPhysical memory (bytes) snapshot=12612890624\n",
      "\t\tVirtual memory (bytes) snapshot=182304817152\n",
      "\t\tTotal committed heap usage (bytes)=12479627264\n",
      "\t\tPeak Map Physical memory (bytes)=355938304\n",
      "\t\tPeak Map Virtual memory (bytes)=4342722560\n",
      "\t\tPeak Reduce Physical memory (bytes)=256258048\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4346028032\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7733101\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=276444\n",
      "2025-04-16 08:01:15,735 INFO streaming.StreamJob: Output directory: /user/ubuntu/nomer1/word_count_task\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## шаблон для запуска MR таски\n",
    "\n",
    "# обязательная чистка директории, куда будем складывать результат отрабоки mr\n",
    "hdfs dfs -rm -r /user/ubuntu/nomer1/word_count_task || true\n",
    "\n",
    "# запус mr таски с указанием пути до нужного jar\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"word-count\" \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer.py\" \\\n",
    "    -input /user/ubuntu/nomer1/bible.txt \\\n",
    "    -output /user/ubuntu/nomer1/word_count_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c0ff5",
   "metadata": {
    "id": "fe9c0ff5"
   },
   "source": [
    "Мониторить процесс работы таски можно на nodemanager по порту 8088 (уже прокинут в конфиге), там будет UI, в котором будет видно вашу запущенную задачу и её статус."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62cdb6",
   "metadata": {
    "id": "4f62cdb6"
   },
   "source": [
    "Результат работы скрипта должен выглядеть следующим образом (вывод тестовый):\n",
    "\n",
    "```bash\n",
    "word count\n",
    "abtr 6852\n",
    "btoad 4237\n",
    "stress 1932\n",
    "zen 1885\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "868c1f67",
   "metadata": {
    "id": "868c1f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lord.\t674\n",
      "pray.\t5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## запустите эту команду, чтобы вывести счетчик определенных слов, которые мы указали на grep\n",
    "## Это нам будет необходимо для визуального анализа результата работы вашего скрипта\n",
    "## в sort можете указать тот разделитель колонок, с которым у вас результат выплевывает редьюсер\n",
    "\n",
    "hdfs dfs -cat /user/ubuntu/nomer1/word_count_task/* | grep  -E 'lord\\.|god\\.|pray\\.' | sort -t$'\\t' -k2.2nr  | head -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511d296",
   "metadata": {
    "id": "1511d296"
   },
   "source": [
    "**2.2 Решаем задачу поиска самых посещаемых сайтов**\n",
    "\n",
    "В данном задании нужно поработать с логом данных о посещении юзерами различных сайтов.\n",
    "Формат данных: `url;временная метка`. Вам нужно вывести топ 5 сайтов по посещаемости в каждую из дат, которая представлена в наших данных.\n",
    "\n",
    "Результат работы скрипта должен выглядеть следующим образом:\n",
    "\n",
    "```bash\n",
    "date        site                            count\n",
    "2024-05-25  https://gonzales-bautista.com/  987\n",
    "2024-05-25  https://smith.com/              654\n",
    "2024-05-25  https://www.smith.com/          321\n",
    "```\n",
    "\n",
    "**Рекомендации**\n",
    "\n",
    "1. Вам могу пригодиться дополнительные параметры mr таски, отвечающие за настройку шаффла, и правил сортировки ключей внутри него. Почитать о примерах их использования можно [здесь](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#More_Usage_Examples).\n",
    "\n",
    "2. Не рекомендуем использовать `\\t` в качестве символа разделителя для сложного ключа (потому что по дефолту таб используется для разделения колонок данных, и ключом в таком случае будет только первая колонка до таба). Если вы будете собирать сложный ключ для нужной вам сортировки данных, лучше всего будет использовать другие симловы, к примеру `+, =`.\n",
    "\n",
    "3. Возможно, у вас не получится решить данную задачу за одну mr таску, тогда вы просто описываете в решении скрипты ваших мапперов, редьюсеров под каждую из mr тасок, которые вам нужно запустить для получения нужного результата.\n",
    "\n",
    "**Важно** помнить, что любой маппер и редьюсер должен работать за O(1) памяти, и если вы будете создавать какой-то список, куда будете складывать какие-то данные, то он не должен быть размера O(n). Если такой момент в вашем решении будет, пожалуйста, поясните его текстово, что с вашими переменными все ок, и у них нет размера O(n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe909b",
   "metadata": {
    "id": "0cfe909b"
   },
   "outputs": [],
   "source": [
    "## первая вариант решения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbbc9b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "import sys\n",
    "\n",
    "for stroka in sys.stdin:\n",
    "    stroka = stroka.strip()\n",
    "    if not stroka:\n",
    "        continue\n",
    "    try:\n",
    "        url, time = stroka.split(';')\n",
    "        date = time.split(' ')[0]\n",
    "        print(f\"{date}\\t{url}\\t1\")  \n",
    "    except:\n",
    "        pass  # Игнорируем некорректные строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "300b5b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "import sys\n",
    "\n",
    "ans_date = None\n",
    "ans_url = None\n",
    "ans_count = 0\n",
    "\n",
    "for stroka in sys.stdin:\n",
    "    stroka = stroka.strip()\n",
    "    if not stroka:\n",
    "        continue\n",
    "    stolb = stroka.split('\\t')\n",
    "    if len(stolb) != 3:\n",
    "        continue\n",
    "    date, url, count = stolb[0], stolb[1], int(stolb[2])\n",
    "    \n",
    "    if date == ans_date and url == ans_url:\n",
    "        ans_count += count\n",
    "    else:\n",
    "        if ans_date:\n",
    "            print(f\"{ans_date}\\t{ans_url}\\t{ans_count}\")\n",
    "        ans_date = date\n",
    "        ans_url = url\n",
    "        ans_count = count\n",
    "\n",
    "# Выводим последнюю запись\n",
    "if ans_date:\n",
    "    print(f\"{ans_date}\\t{ans_url}\\t{ans_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5b7f4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\tsite\tcount\n",
      "2024-05-26\thttps://gonzales-bautista.com/\t335\n",
      "2024-05-26\thttp://smith.com/\t235\n",
      "2024-05-26\thttps://www.smith.com/\t221\n",
      "2024-05-26\thttps://smith.com/\t212\n",
      "2024-05-26\thttp://www.smith.com/\t212\n",
      "2024-05-27\thttps://gonzales-bautista.com/\t376\n",
      "2024-05-27\thttps://www.smith.com/\t270\n",
      "2024-05-27\thttps://smith.com/\t236\n",
      "2024-05-27\thttp://smith.com/\t215\n",
      "2024-05-27\thttp://www.smith.com/\t208\n",
      "2024-05-28\thttps://gonzales-bautista.com/\t368\n",
      "2024-05-28\thttps://smith.com/\t256\n",
      "2024-05-28\thttps://www.smith.com/\t251\n",
      "2024-05-28\thttp://smith.com/\t224\n",
      "2024-05-28\thttp://www.smith.com/\t204\n",
      "2024-05-29\thttps://gonzales-bautista.com/\t402\n",
      "2024-05-29\thttps://www.smith.com/\t242\n",
      "2024-05-29\thttp://www.smith.com/\t223\n",
      "2024-05-29\thttps://smith.com/\t220\n",
      "2024-05-29\thttp://smith.com/\t206\n",
      "2024-05-30\thttps://gonzales-bautista.com/\t353\n",
      "2024-05-30\thttps://smith.com/\t246\n",
      "2024-05-30\thttps://www.smith.com/\t239\n",
      "2024-05-30\thttp://smith.com/\t229\n",
      "2024-05-30\thttp://www.smith.com/\t225\n",
      "2024-05-31\thttps://gonzales-bautista.com/\t374\n",
      "2024-05-31\thttps://www.smith.com/\t244\n",
      "2024-05-31\thttp://smith.com/\t228\n",
      "2024-05-31\thttp://www.smith.com/\t221\n",
      "2024-05-31\thttps://smith.com/\t219\n",
      "2024-06-01\thttps://gonzales-bautista.com/\t379\n",
      "2024-06-01\thttps://www.smith.com/\t232\n",
      "2024-06-01\thttps://smith.com/\t226\n",
      "2024-06-01\thttps://johnson.com/\t195\n",
      "2024-06-01\thttp://smith.com/\t191\n",
      "2024-06-02\thttps://gonzales-bautista.com/\t7\n",
      "2024-06-02\thttp://smith.com/\t7\n",
      "2024-06-02\thttps://www.williams.com/\t6\n",
      "2024-06-02\thttp://lee.com/\t5\n",
      "2024-06-02\thttp://miller.com/\t5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Выводим заголовок\n",
    "echo -e \"date\\tsite\\tcount\"\n",
    "# Обрабатываем данные\n",
    "cat Посещения_сайтов.csv | python3 mapper2.py | sort -k1,1 | python3 reducer2.py | sort -k1,1 -k3,3nr | awk '$1 != prev {prev=$1; cnt=1} cnt <=5 {print; cnt++}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f314bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#второе решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14e3a2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.py\n",
    "import sys\n",
    "\n",
    "for stroka in sys.stdin:\n",
    "    try:\n",
    "        url, time = stroka.strip().split(';')\n",
    "        date = time.split(' ')[0]\n",
    "        print(f\"{date}+{url}\\t1\")\n",
    "    except:\n",
    "        # Пропускаем некорректные строки\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f76e839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.py\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "ans_key = None\n",
    "ans_count = 0\n",
    "date_dict = defaultdict(list) #т.к. я буду хранить тут всегда только 5 элементов и это фиксированное число, то память останется o(1)\n",
    "for stroka in sys.stdin:\n",
    "    key, value = stroka.strip().split('\\t')\n",
    "    \n",
    "    if key != ans_key:\n",
    "        if ans_key:\n",
    "            date, url = ans_key.split('+')\n",
    "            # Добавляем в список и поддерживаем топ-5\n",
    "            date_dict[date].append((url, ans_count))\n",
    "            date_dict[date].sort(key=lambda x: -x[1])\n",
    "            date_dict[date] = date_dict[date][:5]\n",
    "        ans_key = key\n",
    "        ans_count = 0\n",
    "    ans_count += int(value)\n",
    "\n",
    "# Обрабатываем последний ключ\n",
    "if ans_key:\n",
    "    date, url = ans_key.split('+')\n",
    "    date_dict[date].append((url, ans_count))\n",
    "    date_dict[date].sort(key=lambda x: -x[1])\n",
    "    date_dict[date] = date_dict[date][:5]\n",
    "\n",
    "for date in sorted(date_dict.keys()):\n",
    "    for url, count in date_dict[date]:\n",
    "        print(f\"{date}\\t{url}\\t{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92b3ddbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\tsite\tcount\n",
      "2024-05-26\thttps://gonzales-bautista.com/\t335\n",
      "2024-05-26\thttp://smith.com/\t235\n",
      "2024-05-26\thttps://www.smith.com/\t221\n",
      "2024-05-26\thttps://smith.com/\t212\n",
      "2024-05-26\thttp://www.smith.com/\t212\n",
      "2024-05-27\thttps://gonzales-bautista.com/\t376\n",
      "2024-05-27\thttps://www.smith.com/\t270\n",
      "2024-05-27\thttps://smith.com/\t236\n",
      "2024-05-27\thttp://smith.com/\t215\n",
      "2024-05-27\thttp://www.smith.com/\t208\n",
      "2024-05-28\thttps://gonzales-bautista.com/\t368\n",
      "2024-05-28\thttps://smith.com/\t256\n",
      "2024-05-28\thttps://www.smith.com/\t251\n",
      "2024-05-28\thttp://smith.com/\t224\n",
      "2024-05-28\thttp://www.smith.com/\t204\n",
      "2024-05-29\thttps://gonzales-bautista.com/\t402\n",
      "2024-05-29\thttps://www.smith.com/\t242\n",
      "2024-05-29\thttp://www.smith.com/\t223\n",
      "2024-05-29\thttps://smith.com/\t220\n",
      "2024-05-29\thttp://smith.com/\t206\n",
      "2024-05-30\thttps://gonzales-bautista.com/\t353\n",
      "2024-05-30\thttps://smith.com/\t246\n",
      "2024-05-30\thttps://www.smith.com/\t239\n",
      "2024-05-30\thttp://smith.com/\t229\n",
      "2024-05-30\thttp://www.smith.com/\t225\n",
      "2024-05-31\thttps://gonzales-bautista.com/\t374\n",
      "2024-05-31\thttps://www.smith.com/\t244\n",
      "2024-05-31\thttp://smith.com/\t228\n",
      "2024-05-31\thttp://www.smith.com/\t221\n",
      "2024-05-31\thttps://smith.com/\t219\n",
      "2024-06-01\thttps://gonzales-bautista.com/\t379\n",
      "2024-06-01\thttps://www.smith.com/\t232\n",
      "2024-06-01\thttps://smith.com/\t226\n",
      "2024-06-01\thttps://johnson.com/\t195\n",
      "2024-06-01\thttp://smith.com/\t191\n",
      "2024-06-02\thttps://gonzales-bautista.com/\t7\n",
      "2024-06-02\thttp://smith.com/\t7\n",
      "2024-06-02\thttps://www.williams.com/\t6\n",
      "2024-06-02\thttp://lee.com/\t5\n",
      "2024-06-02\thttp://miller.com/\t5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Выводим заголовок\n",
    "echo -e \"date\\tsite\\tcount\"\n",
    "# Обрабатываем данные\n",
    "cat Посещения_сайтов.csv | python3 mapper3.py | sort -k1,1 | python3 reducer3.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9456a3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob3425796163214863680.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `/user/ubuntu/nomer2/url_task_2': No such file or directory\n",
      "2025-04-16 09:21:12,859 INFO client.RMProxy: Connecting to ResourceManager at rc1d-dataproc-m-8hvn1jfvk1pddbqe.mdb.yandexcloud.net/10.130.0.11:8032\n",
      "2025-04-16 09:21:13,093 INFO client.AHSProxy: Connecting to Application History server at rc1d-dataproc-m-8hvn1jfvk1pddbqe.mdb.yandexcloud.net/10.130.0.11:10200\n",
      "2025-04-16 09:21:13,137 INFO client.RMProxy: Connecting to ResourceManager at rc1d-dataproc-m-8hvn1jfvk1pddbqe.mdb.yandexcloud.net/10.130.0.11:8032\n",
      "2025-04-16 09:21:13,138 INFO client.AHSProxy: Connecting to Application History server at rc1d-dataproc-m-8hvn1jfvk1pddbqe.mdb.yandexcloud.net/10.130.0.11:10200\n",
      "2025-04-16 09:21:13,343 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1744790037149_0012\n",
      "2025-04-16 09:21:13,659 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-04-16 09:21:13,756 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2025-04-16 09:21:13,927 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1744790037149_0012\n",
      "2025-04-16 09:21:13,928 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-04-16 09:21:14,139 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-04-16 09:21:14,139 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-04-16 09:21:14,226 INFO impl.YarnClientImpl: Submitted application application_1744790037149_0012\n",
      "2025-04-16 09:21:14,272 INFO mapreduce.Job: The url to track the job: http://rc1d-dataproc-m-8hvn1jfvk1pddbqe.mdb.yandexcloud.net:8088/proxy/application_1744790037149_0012/\n",
      "2025-04-16 09:21:14,277 INFO mapreduce.Job: Running job: job_1744790037149_0012\n",
      "2025-04-16 09:21:19,343 INFO mapreduce.Job: Job job_1744790037149_0012 running in uber mode : false\n",
      "2025-04-16 09:21:19,345 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-04-16 09:21:25,409 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2025-04-16 09:21:26,415 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "2025-04-16 09:21:27,423 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "2025-04-16 09:21:28,429 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2025-04-16 09:21:29,434 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2025-04-16 09:21:33,454 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2025-04-16 09:21:34,462 INFO mapreduce.Job:  map 47% reduce 0%\n",
      "2025-04-16 09:21:36,473 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2025-04-16 09:21:39,489 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "2025-04-16 09:21:40,493 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2025-04-16 09:21:41,499 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2025-04-16 09:21:42,505 INFO mapreduce.Job:  map 73% reduce 0%\n",
      "2025-04-16 09:21:43,509 INFO mapreduce.Job:  map 77% reduce 0%\n",
      "2025-04-16 09:21:44,515 INFO mapreduce.Job:  map 77% reduce 24%\n",
      "2025-04-16 09:21:47,530 INFO mapreduce.Job:  map 87% reduce 24%\n",
      "2025-04-16 09:21:48,535 INFO mapreduce.Job:  map 100% reduce 24%\n",
      "2025-04-16 09:21:50,546 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-04-16 09:21:51,556 INFO mapreduce.Job: Job job_1744790037149_0012 completed successfully\n",
      "2025-04-16 09:21:51,637 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1934223\n",
      "\t\tFILE: Number of bytes written=13285795\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=39223142\n",
      "\t\tHDFS: Number of bytes written=1490\n",
      "\t\tHDFS: Number of read operations=95\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=16\n",
      "\t\tRack-local map tasks=14\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=382047\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=62001\n",
      "\t\tTotal time spent by all map tasks (ms)=127349\n",
      "\t\tTotal time spent by all reduce tasks (ms)=20667\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=127349\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=20667\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=391216128\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=63489024\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=700000\n",
      "\t\tMap output records=700000\n",
      "\t\tMap output bytes=25943383\n",
      "\t\tMap output materialized bytes=3827876\n",
      "\t\tInput split bytes=5040\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=274260\n",
      "\t\tReduce shuffle bytes=3827876\n",
      "\t\tReduce input records=700000\n",
      "\t\tReduce output records=40\n",
      "\t\tSpilled Records=1400000\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=3237\n",
      "\t\tCPU time spent (ms)=42030\n",
      "\t\tPhysical memory (bytes) snapshot=9972584448\n",
      "\t\tVirtual memory (bytes) snapshot=134533775360\n",
      "\t\tTotal committed heap usage (bytes)=9774825472\n",
      "\t\tPeak Map Physical memory (bytes)=363667456\n",
      "\t\tPeak Map Virtual memory (bytes)=4344999936\n",
      "\t\tPeak Reduce Physical memory (bytes)=225157120\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4359532544\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=39218102\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1490\n",
      "2025-04-16 09:21:51,637 INFO streaming.StreamJob: Output directory: /user/ubuntu/nomer2/url_task_2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Очистка предыдущих результатов в HDFS (если есть)\n",
    "hdfs dfs -rm -r /user/ubuntu/nomer2/url_task_2 || true\n",
    "\n",
    "# Запуск MapReduce задачи через Hadoop Streaming\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"url_task_2\" \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -files mapper3.py,reducer3.py \\\n",
    "    -mapper \"python3 mapper3.py\" \\\n",
    "    -reducer \"python3 reducer3.py\" \\\n",
    "    -input /user/ubuntu/nomer2/Посещения_сайтов.csv \\\n",
    "    -output /user/ubuntu/nomer2/url_task_2\n",
    "    \n",
    "    # добавила строчку \"-D mapreduce.job.reduces=1 \\\" чтобы сделать вывод корректным, но по факту \n",
    "    # снизила производительность, на нашем объеме данных это не сильно влияет, но на больших данных надо думать \n",
    "    # и реализовывать что-то более сложное. Он был некорректым потому что в трех нодах были даты которые нам нужны и \n",
    "    # и он считал в каждой ноде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e795737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-28  https://gonzales-bautista.com/  368\n",
      "2024-05-28  https://smith.com/              256\n",
      "2024-05-28  https://www.smith.com/          251\n",
      "2024-05-28  http://smith.com/               224\n",
      "2024-05-28  http://www.smith.com/           204\n",
      "2024-05-30  https://gonzales-bautista.com/  353\n",
      "2024-05-30  https://smith.com/              246\n",
      "2024-05-30  https://www.smith.com/          239\n",
      "2024-05-30  http://smith.com/               229\n",
      "2024-05-30  http://www.smith.com/           225\n",
      "2024-06-02  http://smith.com/               7\n",
      "2024-06-02  https://gonzales-bautista.com/  7\n",
      "2024-06-02  https://www.williams.com/       6\n",
      "2024-06-02  http://lee.com/                 5\n",
      "2024-06-02  http://miller.com/              5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -cat /user/ubuntu/nomer2/url_task_2/* | grep -E '2024-05-28|2024-06-02|2024-05-30' | column -t -s$'\\t' "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
